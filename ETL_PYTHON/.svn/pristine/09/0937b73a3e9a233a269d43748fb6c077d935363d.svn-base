import gc
import logging
import os
import shutil
import time
from datetime import datetime

import duckdb
import pandas as pd
import numpy as np

from xinxiang.util import my_duck, my_oracle, my_date, cons_error_code, my_postgres, my_file
from xinxiang import config


def HandlingVerControlOracle(conn, Uuid, TableName, Partcode):
    timeStr = my_date.date_time_second_short_str()
    sql = """
                        INSERT INTO aps_etl_ver_control
                        ( ID, MODULE_ID, TOOLG_ID, TABLE_NAME, UPDATE_TIME, UPDATE_USER, PARTCODE )
                        VALUES('{}', '' , '', '{}', '{}', '{}', '{}') 
                        """.format(
        Uuid,
        TableName,
        timeStr,
        "CIM",
        Partcode
    )
    dbcursor = conn.cursor()
    dbcursor.execute(sql)


def GetNextPartCodeData(conn, TableName):
    next = None
    curr_partcode = my_oracle.GetLastPartCodeData(conn, TableName)
    if curr_partcode == '':
        next = '2000'
    elif curr_partcode == '2000':
        next = '4000'
    elif curr_partcode == '4000':
        next = '6000'
    elif curr_partcode == '6000':
        next = '2000'
    return next

def GetPartName(partcode):
    if partcode == '2000':
        return "PART01"
    elif partcode == '4000':
        return "PART02"
    elif partcode == '6000':
        return "PART03"
    return None

def _execute(table_name):
    try:
        _execute2(table_name)
    except Exception as e:
        logging.error("回写失败，文件被打开,休眠120秒后再执行")
        time.sleep(120)
        try:
            _execute2(table_name)
        except Exception as ee:
            logging.error("回写失败，文件被打开,休眠120秒后再执行")
            time.sleep(120)
            _execute2(table_name)

def _execute2(table_name):
    oracle_conn = my_oracle.oracle_get_connection()
    if config.g_debug_mode:
        oracle_conn = my_oracle.oracle_get_connection_local()
    csv_file_name = None
    dbcursor = None
    parentid = None
    try:
        traget_file_name = my_file.get_last_db_file(oracle_conn, table_name)
        duck_db = duckdb.connect(database=':memory:')
        sql = "ATTACH '{file_name}' AS {table_name} (READ_ONLY) ".format(file_name=traget_file_name,
                                                                         table_name=table_name)
        duck_db.sql(sql)

        folder = os.path.join(config.g_mem_etl_output_path, 'temp')
        if not os.path.exists(folder):
            os.makedirs(folder)

        uuid = my_oracle.UUID()
        csv_file_name = os.path.join(folder, table_name + '_' + uuid + '.csv')
        current_time_short = my_date.date_time_second_str()

        part_code = GetNextPartCodeData(oracle_conn, TableName=table_name)
        print(csv_file_name, '>', part_code)

        part_name = GetPartName(part_code)

        if part_name:
            # delete_sql = """ALTER TABLE {table_name} TRUNCATE PARTITION ({part_name}) UPDATE INDEXES""".format(
            #     table_name=table_name, part_name=part_name)
            # delete_sql = """TRUNCATE table {table_name} where partcode='{partcode}' """.format(table_name=table_name, partcode=part_code)
            # print(delete_sql)
            delete_sql = """delete from {table_name} where partcode='{part_code}'""".format(table_name=table_name, part_code=part_code)
            logging.info(delete_sql)
            dbcursor = oracle_conn.cursor()
            dbcursor.execute(delete_sql)

            date_type_sql = f"select column_name, data_type from all_tab_columns where upper(table_name) = '{table_name}'"
            column_types = pd.read_sql_query(date_type_sql, oracle_conn)

            total_size = my_duck.get_row_count_in_duckdb(duck_db, tableName=table_name + "." + table_name)
            print(f"{table_name} >>> 读取{total_size}行 >>> partcode >>> {part_code}")
            batch_size = 100000

            for offset in range(0, total_size, batch_size):
                query_sql = """select * from {table_name}.{table_name} limit {batch_size} offset {offset}""".format(
                    table_name=table_name, batch_size=batch_size, offset=offset)
                df = pd.read_sql_query(query_sql, duck_db)
                df.columns = df.columns.str.lower()
                # df['parentid'] = uuid

                parentid = df.loc[0, 'parentid']
                print(parentid)
                df['sync_time'] = current_time_short
                df['partcode'] = part_code

                column_types['COLUMN_NAME'] = column_types['COLUMN_NAME'].str.lower()
                column_types['DATA_TYPE'] = column_types['DATA_TYPE'].str.lower()

                df = df.replace(pd.NaT, np.nan)

                for col in df.columns:
                    aaa = column_types[column_types['COLUMN_NAME'].str.contains(col)]
                    if aaa.shape[0] > 0:
                        if column_types[column_types['COLUMN_NAME'] == col]['DATA_TYPE'].values[0] == 'date':
                            try:
                                df[col] = pd.to_datetime(df[col])
                                df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M')
                            except Exception as e:
                                try:
                                    df[col].fillna("", inplace=True)
                                except Exception as eee:
                                    try:
                                        print("---------------------------!!!!!")
                                        df[col].fillna('1970-01-01 00:00:00', inplace=True)
                                        df[col] = pd.to_datetime(df[col], format='mixed')
                                        df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M')
                                    except Exception as fff:
                                        df[col].fillna("'null'", inplace=True)
                        if column_types[column_types['COLUMN_NAME'] == col]['DATA_TYPE'].values[0] == 'number':
                            df[col].fillna(0, inplace=True)
                            df[col] = pd.to_numeric(df[col])
                        if column_types[column_types['COLUMN_NAME'] == col]['DATA_TYPE'].values[0].startswith('varchar'):
                            df[col].fillna('', inplace=True)

                    else:
                        # Duckdb有，Oracle中不存在这列的时候
                        del df[col]

                # df.to_sql(table_name, oracle_conn, if_exists='replace', index=False)
                insert_sql = f"""insert into {table_name} ({', '.join(df.columns)}) values( {', '.join([':' + str(i + 1) for i in range(len(df.columns))])} )"""
                print(df.shape)
                data_list = [tuple(row) for row in df.to_numpy()]
                print("111::::", len(data_list[0]))
                print(insert_sql)
                try:
                    dbcursor.executemany(insert_sql, data_list)
                except Exception as ee:
                    print("====================================================")
                    print(ee)
                    print(data_list[0])

                start_row = offset + 1
                end_row = min(offset + batch_size, total_size)
                print(f"........读取{start_row}到{end_row}行")
                df = None

        HandlingVerControlOracle(conn=oracle_conn, Uuid=parentid, TableName=table_name, Partcode=part_code)

        duck_db.close()
        logging.info("""Write back success:{table_name}""".format(table_name=table_name))
    except Exception as e:
        logging.error(e)
        print(e)
        raise e
    finally:
        if dbcursor:
            dbcursor.close()
        if oracle_conn:
            oracle_conn.commit()
            oracle_conn.close()
        if os.path.exists(csv_file_name):
            os.remove(csv_file_name)

# ======================================
# Phase 1
def execute_flow():
    table_name = 'APS_ETL_FLOW'
    _execute(table_name)

def execute_wip():
    table_name = 'APS_ETL_WIP'
    _execute(table_name)

# ======================================
# Phase 2
# def execute_tool_down():
#     table_name = 'APS_ETL_TOOL_DOWN'
#     _execute(table_name)
#
# def execute_qtime_spec():
#     table_name = 'APS_ETL_QTIME_SPEC'
#     _execute(table_name)
#
# def execute_qtime_hold():
#     table_name = 'APS_ETL_QTIME_HOLD'
#     _execute(table_name)
#
# def execute_wafer_start():
#     table_name = 'APS_ETL_WAFER_START'
#     _execute(table_name)
#
# def execute_vc_record():
#     table_name = 'APS_ETL_VC_RECORD'
#     _execute(table_name)
#
# def execute_transport():
#     table_name = 'APS_ETL_TRANSPORT'
#     _execute(table_name)
#
# def execute_etl_tool():
#     table_name = 'APS_ETL_TOOL'
#     _execute(table_name)
#
# def execute_size_control():
#     table_name = 'APS_ETL_SIZE_CONTROL'
#     _execute(table_name)
#
# def execute_season_rule():
#     table_name = 'APS_ETL_SEASON_RULE'
#     _execute(table_name)
#
# def execute_sampling_lot():
#     table_name = 'APS_ETL_SAMPLING_LOT'
#     _execute(table_name)
#
# def execute_ppid_info():
#     table_name = 'APS_ETL_PPID_INFO'
#     _execute(table_name)
#
# def execute_hold_rls():
#     table_name = 'APS_ETL_HOLD_RLS'
#     _execute(table_name)

# ======================================
# Phase 3
# def execute_rls():
#     table_name = 'APS_ETL_RLS'
#     _execute(table_name)

# def execute_pri_wip():
#     table_name = 'APS_ETL_PRI_WIP'
#     _execute(table_name)

# def execute_monitor():
#     table_name = 'APS_ETL_MONITOR'
#     _execute(table_name)

# def execute_lothistory():
#     table_name = 'APS_ETL_LOTHISTORY'
#     _execute(table_name)

# def execute_lot_op_hist():
#     table_name = 'APS_ETL_LOT_OP_HIST'
#     _execute(table_name)

# def execute_mask_history():
#     table_name = 'APS_ETL_MASK_HISTORY'
#     _execute(table_name)

# def execute_sgs_rls():
#     table_name = 'APS_ETL_SGS_RLS'
#     _execute(table_name)

# def execute_pirun():
#     table_name = 'APS_ETL_PIRUN'
#     _execute(table_name)

# def execute_sgs_rule():
#     table_name = 'APS_ETL_SGS_RULE'
#     _execute(table_name)

# def execute_mask_info():
#     table_name = 'APS_ETL_MASK_INFO'
#     _execute(table_name)

# def execute_demand():
#     table_name = 'APS_ETL_DEMAND'
#     _execute(table_name)


if __name__ == '__main__':
    execute_wip()
